#!/usr/bin/env python3
"""
Precomputed Dataset classes for loading data from disk instead of generating on-the-fly.

These classes replace the IterableDataset implementations with regular Dataset classes
that load precomputed data from .npz files generated by generate_precomputed_data.py
"""

import glob
import os
import re
from typing import Dict, List, Optional, Tuple

import numpy as np
import torch
from torch.utils.data import DataLoader, Dataset

np.random.seed(42)
torch.manual_seed(42)

def parse_filename_parameters(filename: str) -> Optional[Dict[str, int]]:
    """
    Parse parameters from a precomputed data filename.

    Expected format: {problem}_ns{num_samples}_ne{num_events}_nr{n_repeat}.npz

    Args:
        filename: The filename to parse (just the basename, not full path)

    Returns:
        Dictionary with 'ns', 'ne', 'nr' values, or None if parsing fails
    """
    # Remove .npz extension
    basename = filename.replace(".npz", "")

    # Pattern to match: problem_ns{digits}_ne{digits}_nr{digits}
    pattern = r"^(.+)_ns(\d+)_ne(\d+)_nr(\d+)$"
    match = re.match(pattern, basename)

    if match:
        problem = match.group(1)
        ns = int(match.group(2))
        ne = int(match.group(3))
        nr = int(match.group(4))
        return {"problem": problem, "ns": ns, "ne": ne, "nr": nr}
    return None


def filter_files_by_exact_parameters(
    file_list: List[str], required_ns: int, required_ne: int, required_nr: int
) -> List[str]:
    """
    Filter files to only include those with exact parameter matches.

    Args:
        file_list: List of file paths to filter
        required_ns: Required num_samples
        required_ne: Required num_events
        required_nr: Required n_repeat

    Returns:
        List of files that match the exact parameters
    """
    exact_matches = []
    print(
        f"   üîç FILTERING FOR EXACT PARAMETERS: ns={required_ns}, ne={required_ne}, nr={required_nr}"
    )

    for file_path in file_list:
        filename = os.path.basename(file_path)
        params = parse_filename_parameters(filename)

        if params is None:
            print(f"     ‚úó SKIP: '{filename}' - Cannot parse parameters")
            continue

        matches = (
            params["ns"] == required_ns
            and params["ne"] == required_ne
            and params["nr"] == required_nr
        )

        status = "‚úì MATCH" if matches else "‚úó NO MATCH"
        print(
            f"     {status}: '{filename}' - ns={params['ns']}, ne={params['ne']}, nr={params['nr']}"
        )

        if matches:
            exact_matches.append(file_path)

    print(
        f"   üìä EXACT MATCH RESULT: {len(exact_matches)} files match out of {len(file_list)} total"
    )
    return exact_matches


def filter_valid_precomputed_files(file_list: List[str]) -> List[str]:
    """
    Filter a list of files to only include valid precomputed dataset files.

    Valid files must:
    - End with '.npz'
    - Not contain '.tmp' in the filename (excludes temporary/incomplete files)

    Args:
        file_list: List of file paths to filter

    Returns:
        List of valid precomputed dataset file paths
    """
    valid_files = []
    print(f"   üîç FILTERING {len(file_list)} FILES:")

    for file_path in file_list:
        filename = os.path.basename(file_path)
        ends_with_npz = filename.endswith(".npz")
        has_tmp = ".tmp" in filename
        is_valid = ends_with_npz and not has_tmp

        status = "‚úì VALID" if is_valid else "‚úó INVALID"
        reason = []
        if not ends_with_npz:
            reason.append("not .npz")
        if has_tmp:
            reason.append("contains .tmp")
        reason_str = f" ({', '.join(reason)})" if reason else ""

        print(f"     {status}: '{filename}'{reason_str}")

        # Only include .npz files that don't contain '.tmp' anywhere in the filename
        if is_valid:
            valid_files.append(file_path)

    print(
        f"   üìä FILTERING RESULT: {len(valid_files)} valid out of {len(file_list)} total files"
    )
    return valid_files


class PrecomputedDataset(Dataset):
    """
    Dataset class for loading precomputed theta parameters and corresponding event data.

    This replaces the on-the-fly data generation with disk-based loading for:
    - Better reproducibility
    - Faster training iterations
    - Easier debugging

    Supports both:
    - Single .npz file input (loads that specific file)
    - Directory input (searches for matching files in the directory)
    """

    def __init__(
        self,
        data_dir: str,
        problem: str,
        shuffle: bool = True,
        exact_ns: Optional[int] = None,
        exact_ne: Optional[int] = None,
        exact_nr: Optional[int] = None,
    ):
        """
        Initialize dataset from precomputed data files.

        Args:
            data_dir: Directory containing .npz files OR path to a single .npz file
            problem: Problem type ('simplified_dis', 'mceg')
            shuffle: Whether to shuffle the dataset indices
            exact_ns: If specified, only load files with this exact num_samples
            exact_ne: If specified, only load files with this exact num_events
            exact_nr: If specified, only load files with this exact n_repeat
        """
        print("üîç PRECOMPUTED DATASET INITIALIZATION:")
        print(f"   Data path: '{data_dir}'")
        print(f"   Problem type: '{problem}'")
        print(f"   Shuffle: {shuffle}")
        if exact_ns is not None or exact_ne is not None or exact_nr is not None:
            print(
                f"   Exact parameter matching: ns={exact_ns}, ne={exact_ne}, nr={exact_nr}"
            )

        self.data_dir = data_dir
        self.problem = problem
        self.shuffle = shuffle
        self.exact_ns = exact_ns
        self.exact_ne = exact_ne
        self.exact_nr = exact_nr

        # Check if data_dir is a file or directory
        if os.path.isfile(data_dir):
            print(f"   üìÑ Detected single file input: '{data_dir}'")
            # Validate the file
            if not data_dir.endswith(".npz"):
                raise ValueError(
                    f"Single file input must be a .npz file, got: {data_dir}"
                )

            if not os.path.exists(data_dir):
                raise FileNotFoundError(f"Precomputed data file not found: {data_dir}")

            # For single file input, verify exact parameter matching if specified
            if exact_ns is not None or exact_ne is not None or exact_nr is not None:
                filename = os.path.basename(data_dir)
                params = parse_filename_parameters(filename)
                if params is None:
                    raise ValueError(
                        f"Cannot parse parameters from filename: {filename}"
                    )

                # Check each specified parameter
                mismatches = []
                if exact_ns is not None and params["ns"] != exact_ns:
                    mismatches.append(f"ns={params['ns']} (required {exact_ns})")
                if exact_ne is not None and params["ne"] != exact_ne:
                    mismatches.append(f"ne={params['ne']} (required {exact_ne})")
                if exact_nr is not None and params["nr"] != exact_nr:
                    mismatches.append(f"nr={params['nr']} (required {exact_nr})")

                if mismatches:
                    raise ValueError(
                        f"File {filename} has mismatched parameters: {', '.join(mismatches)}"
                    )

                print(
                    f"   ‚úì File matches exact parameters: ns={params['ns']}, ne={params['ne']}, nr={params['nr']}"
                )

            # Use the single file directly
            all_data_files = [data_dir]
            print(f"   Using single file: {data_dir}")

        elif os.path.isdir(data_dir):
            print(f"   üìÅ Detected directory input: '{data_dir}'")
            # Find all matching data files in directory
            pattern = os.path.join(data_dir, f"{problem}_*.npz")
            print(f"   Search pattern: '{pattern}'")
            all_data_files = sorted(glob.glob(pattern))
            print(
                f"   Found {len(all_data_files)} files matching pattern: {[os.path.basename(f) for f in all_data_files]}"
            )

        else:
            raise FileNotFoundError(
                f"Data path does not exist or is neither a file nor directory: {data_dir}"
            )

        # Filter out temporary/incomplete files
        valid_files = filter_valid_precomputed_files(all_data_files)
        print(
            f"   After filtering invalid files: {len(valid_files)} valid files: {[os.path.basename(f) for f in valid_files]}"
        )

        # Apply exact parameter filtering if specified (only for directory mode)
        if os.path.isdir(data_dir) and (
            exact_ns is not None or exact_ne is not None or exact_nr is not None
        ):
            if exact_ns is None or exact_ne is None or exact_nr is None:
                raise ValueError(
                    "When using exact parameter matching with directory input, all parameters (exact_ns, exact_ne, exact_nr) must be specified"
                )

            exact_matches = filter_files_by_exact_parameters(
                valid_files, exact_ns, exact_ne, exact_nr
            )
            self.data_files = exact_matches

            if not self.data_files:
                print(
                    f"   ‚úó FAILURE: No files match exact parameters ns={exact_ns}, ne={exact_ne}, nr={exact_nr}"
                )

                # Show what files were available to help debugging
                if valid_files:
                    print(f"   üìã Available files for problem '{problem}':")
                    for file_path in valid_files:
                        filename = os.path.basename(file_path)
                        params = parse_filename_parameters(filename)
                        if params:
                            print(
                                f"      - {filename} (ns={params['ns']}, ne={params['ne']}, nr={params['nr']})"
                            )
                        else:
                            print(f"      - {filename} (cannot parse parameters)")

                expected_filename = (
                    f"{problem}_ns{exact_ns}_ne{exact_ne}_nr{exact_nr}.npz"
                )
                error_msg = (
                    f"No precomputed data files found with exact parameters: ns={exact_ns}, ne={exact_ne}, nr={exact_nr}\n"
                    f"Expected file: {expected_filename}\n"
                    f"Searched in: {data_dir}"
                )
                raise FileNotFoundError(error_msg)
        else:
            # No exact parameter filtering - use all valid files
            self.data_files = valid_files
        print(
            f"   After all filtering: {len(self.data_files)} final files: {[os.path.basename(f) for f in self.data_files]}"
        )

        if not self.data_files:
            if os.path.isdir(data_dir):
                if valid_files and (
                    exact_ns is not None or exact_ne is not None or exact_nr is not None
                ):
                    # Exact parameter matching failed
                    print(
                        f"   ‚úó FAILURE: No files match exact parameters for problem '{problem}'"
                    )
                elif all_data_files and not valid_files:
                    # Only temporary files found
                    temp_files = [f for f in all_data_files if f not in valid_files]
                    print(
                        f"   ‚ö†Ô∏è  Warning: Found {len(temp_files)} temporary/incomplete files for problem '{problem}' in {data_dir}: {[os.path.basename(f) for f in temp_files]}"
                    )
                    print(
                        f"   üìÅ Only complete .npz files (without .tmp) are considered valid precomputed data."
                    )
                    print(
                        f"   üí° To fix: Remove '.tmp' from complete files or regenerate data without .tmp suffix"
                    )
                    print(
                        f"   ‚úó FAILURE: No valid precomputed data files found for problem '{problem}' in {data_dir}"
                    )
                else:
                    # No files found for this problem at all
                    print(
                        f"   ‚úó FAILURE: No precomputed data files found for problem '{problem}' in {data_dir}"
                    )
            print()
            raise FileNotFoundError(
                f"No valid precomputed data files found for problem '{problem}' in {data_dir}"
            )

        print(
            f"   ‚úì SUCCESS: Found {len(self.data_files)} valid data files for {problem}"
        )
        if os.path.isdir(data_dir):
            if len(all_data_files) > len(self.data_files):
                ignored_files = [f for f in all_data_files if f not in self.data_files]
                print(
                    f"   üóëÔ∏è  Ignored {len(ignored_files)} files: {[os.path.basename(f) for f in ignored_files]}"
                )
        print()

        # Load all data into memory (assuming datasets are not too large)
        self._load_data()

        # Create indices for shuffling
        self.indices = np.arange(len(self.thetas))
        if self.shuffle:
            np.random.shuffle(self.indices)

    def _load_data(self):
        """Load all data files into memory."""
        all_thetas = []
        all_events = []
        file_metadata = []

        print(f"üìÅ LOADING DATA FROM {len(self.data_files)} FILES:")

        for data_file in self.data_files:
            try:
                data = np.load(data_file)
                filename = os.path.basename(data_file)

                # Verify this is the correct problem type
                if "problem" in data:
                    file_problem = str(data["problem"].item())
                    if file_problem != self.problem:
                        print(
                            f"   ‚ö†Ô∏è  WARNING: File {filename} is for problem '{file_problem}', expected '{self.problem}' - SKIPPING"
                        )
                        continue

                thetas = data["thetas"]  # [num_samples, theta_dim]
                events = data[
                    "events"
                ]  # [num_samples, n_repeat, num_events, feature_dim]

                # Parse parameters from filename for validation
                params = parse_filename_parameters(filename)
                if params:
                    file_metadata.append(
                        {
                            "filename": filename,
                            "ns": params["ns"],
                            "ne": params["ne"],
                            "nr": params["nr"],
                            "thetas_shape": thetas.shape,
                            "events_shape": events.shape,
                        }
                    )
                    print(
                        f"   ‚úì Loaded {filename}: ns={params['ns']}, ne={params['ne']}, nr={params['nr']}"
                    )
                    print(
                        f"     Data shapes: thetas {thetas.shape}, events {events.shape}"
                    )
                else:
                    file_metadata.append(
                        {
                            "filename": filename,
                            "thetas_shape": thetas.shape,
                            "events_shape": events.shape,
                        }
                    )
                    print(f"   ‚úì Loaded {filename} (cannot parse parameters)")
                    print(
                        f"     Data shapes: thetas {thetas.shape}, events {events.shape}"
                    )

                all_thetas.append(thetas)
                all_events.append(events)

            except Exception as e:
                print(f"   ‚úó ERROR loading {os.path.basename(data_file)}: {e}")
                continue

        if not all_thetas:
            raise ValueError(f"No valid data loaded for problem {self.problem}")

        # Validate compatibility before concatenating
        print(f"\nüîç VALIDATING DATA COMPATIBILITY:")
        if len(all_thetas) > 1:
            # Check if all files have compatible dimensions
            first_events_shape = all_events[0].shape
            first_theta_shape = all_thetas[0].shape

            compatible = True
            for i, (events, thetas) in enumerate(
                zip(all_events[1:], all_thetas[1:]), 1
            ):
                # Check theta dimensions (should match except for first dimension)
                if thetas.shape[1:] != first_theta_shape[1:]:
                    print(
                        f"   ‚úó INCOMPATIBLE: File {i} theta shape {thetas.shape} vs file 0 {first_theta_shape}"
                    )
                    compatible = False

                # Check events dimensions (should match except for first dimension)
                if events.shape[1:] != first_events_shape[1:]:
                    print(
                        f"   ‚úó INCOMPATIBLE: File {i} events shape {events.shape} vs file 0 {first_events_shape}"
                    )
                    compatible = False

            if not compatible:
                print(f"\n‚ùå CONCATENATION ERROR: Files have incompatible dimensions!")
                print(f"Files loaded:")
                for metadata in file_metadata:
                    filename = metadata["filename"]
                    if "ns" in metadata:
                        print(
                            f"  - {filename}: ns={metadata['ns']}, ne={metadata['ne']}, nr={metadata['nr']}"
                        )
                    else:
                        print(f"  - {filename}")
                    print(
                        f"    thetas: {metadata['thetas_shape']}, events: {metadata['events_shape']}"
                    )

                error_msg = (
                    "Cannot concatenate files with incompatible dimensions. "
                    "This typically happens when files have different num_events, n_repeat, or feature dimensions. "
                    "Use exact parameter matching (exact_ns, exact_ne, exact_nr) to load only compatible files."
                )
                raise ValueError(error_msg)

            print(f"   ‚úì All {len(all_thetas)} files have compatible dimensions")

        # Concatenate all data
        print(f"\nüìä CONCATENATING DATA:")
        self.thetas = np.concatenate(all_thetas, axis=0)
        self.events = np.concatenate(all_events, axis=0)

        print(
            f"   ‚úì Final concatenated data: thetas {self.thetas.shape}, events {self.events.shape}"
        )

        # Convert to tensors
        self.thetas = torch.from_numpy(self.thetas).float()
        self.events = torch.from_numpy(self.events).float()

    def __len__(self):
        return len(self.thetas)

    def __getitem__(self, idx):
        """
        Get a single sample.

        Returns:
            theta: Parameter vector [theta_dim]
            events: Event data [n_repeat, num_events, feature_dim]
        """
        actual_idx = self.indices[idx]
        theta = self.thetas[actual_idx]
        events = self.events[actual_idx]
        return theta, events

    def get_metadata(self):
        """Get metadata about the dataset."""
        return {
            "problem": self.problem,
            "num_samples": len(self.thetas),
            "theta_dim": self.thetas.shape[1],
            "n_repeat": self.events.shape[1],
            "num_events": self.events.shape[2],
            "feature_dim": self.events.shape[3],
            "data_files": self.data_files,
        }


class DistributedPrecomputedDataset(PrecomputedDataset):
    """
    Distributed version of PrecomputedDataset that splits data across ranks.

    This maintains compatibility with the distributed training setup.
    """

    def __init__(
        self,
        data_dir: str,
        problem: str,
        rank: int,
        world_size: int,
        shuffle: bool = True,
        exact_ns: Optional[int] = None,
        exact_ne: Optional[int] = None,
        exact_nr: Optional[int] = None,
    ):
        """
        Initialize distributed dataset.

        Args:
            data_dir: Directory containing .npz files
            problem: Problem type
            rank: Current process rank
            world_size: Total number of processes
            shuffle: Whether to shuffle before splitting
            exact_ns: If specified, only load files with this exact num_samples
            exact_ne: If specified, only load files with this exact num_events
            exact_nr: If specified, only load files with this exact n_repeat
        """
        self.rank = rank
        self.world_size = world_size

        # Load full dataset first
        super().__init__(
            data_dir,
            problem,
            shuffle=shuffle,
            exact_ns=exact_ns,
            exact_ne=exact_ne,
            exact_nr=exact_nr,
        )

        # Split data across ranks
        self._split_data_for_rank()

    def _split_data_for_rank(self):
        """Split the loaded data for this rank."""
        total_samples = len(self.thetas)
        samples_per_rank = total_samples // self.world_size
        remainder = total_samples % self.world_size

        # Calculate start and end indices for this rank
        start_idx = self.rank * samples_per_rank
        end_idx = start_idx + samples_per_rank

        # Distribute remainder among first few ranks
        if self.rank < remainder:
            start_idx += self.rank
            end_idx += self.rank + 1
        else:
            start_idx += remainder
            end_idx += remainder

        # Slice data for this rank
        self.thetas = self.thetas[start_idx:end_idx]
        self.events = self.events[start_idx:end_idx]

        # Update indices
        self.indices = np.arange(len(self.thetas))
        if self.shuffle:
            # Use rank-specific seed for reproducible shuffling
            np.random.seed(42 + self.rank)
            np.random.shuffle(self.indices)

        print(
            f"Rank {self.rank}: samples {start_idx}-{end_idx-1} ({len(self.thetas)} total)"
        )


def create_precomputed_dataloader(
    data_dir: str,
    problem: str,
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 0,
    rank: int = 0,
    world_size: int = 1,
    exact_ns: Optional[int] = None,
    exact_ne: Optional[int] = None,
    exact_nr: Optional[int] = None,
    **kwargs,
) -> DataLoader:
    """
    Create a DataLoader for precomputed data.

    Args:
        data_dir: Directory containing precomputed .npz files OR path to a single .npz file
        problem: Problem type ('gaussian', 'simplified_dis', etc.)
        batch_size: Batch size for training
        shuffle: Whether to shuffle data
        num_workers: Number of worker processes
        rank: Process rank for distributed training
        world_size: Total number of processes
        exact_ns: If specified, only load files with this exact num_samples
        exact_ne: If specified, only load files with this exact num_events
        exact_nr: If specified, only load files with this exact n_repeat
        **kwargs: Additional arguments for DataLoader

    Returns:
        DataLoader instance
    """
    if world_size > 1:
        dataset = DistributedPrecomputedDataset(
            data_dir,
            problem,
            rank,
            world_size,
            shuffle,
            exact_ns=exact_ns,
            exact_ne=exact_ne,
            exact_nr=exact_nr,
        )
    else:
        dataset = PrecomputedDataset(
            data_dir,
            problem,
            shuffle,
            exact_ns=exact_ns,
            exact_ne=exact_ne,
            exact_nr=exact_nr,
        )

    # Don't shuffle in DataLoader if we're handling it in Dataset
    dataloader_shuffle = False

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=dataloader_shuffle,
        num_workers=num_workers,
        pin_memory=True,
        **kwargs,
    )

    return dataloader


def test_precomputed_dataset():
    """Test function for the precomputed dataset classes."""
    import os
    import sys
    import tempfile

    # Add the main directory to path
    sys.path.append("/home/runner/work/PDFParameterInference/PDFParameterInference")

    print("Testing PrecomputedDataset...")

    # Test with the data we generated earlier
    test_dir = "/tmp/test_precomputed"

    if not os.path.exists(test_dir):
        print("No test data found. Run generate_precomputed_data.py first.")
        return

    # Test Gaussian dataset
    try:
        dataset = PrecomputedDataset(test_dir, "gaussian", shuffle=True)
        print(f"Gaussian dataset loaded: {len(dataset)} samples")
        print(f"Metadata: {dataset.get_metadata()}")

        # Test dataloader
        dataloader = DataLoader(dataset, batch_size=4, shuffle=False)
        for i, (theta, events) in enumerate(dataloader):
            print(f"Batch {i}: theta {theta.shape}, events {events.shape}")
            if i >= 1:  # Just test first couple batches
                break

        print("Gaussian dataset test passed!")

    except Exception as e:
        print(f"Gaussian dataset test failed: {e}")

    # Test SimplifiedDIS dataset
    try:
        dataset = PrecomputedDataset(test_dir, "simplified_dis", shuffle=True)
        print(f"SimplifiedDIS dataset loaded: {len(dataset)} samples")
        print(f"Metadata: {dataset.get_metadata()}")

        # Test distributed version
        dist_dataset = DistributedPrecomputedDataset(
            test_dir, "simplified_dis", rank=0, world_size=2
        )
        print(f"Distributed dataset (rank 0/2): {len(dist_dataset)} samples")

        print("SimplifiedDIS dataset test passed!")

    except Exception as e:
        print(f"SimplifiedDIS dataset test failed: {e}")

    print("All tests completed!")


if __name__ == "__main__":
    test_precomputed_dataset()
