#!/usr/bin/env python3
"""
Precomputed Dataset classes for loading data from disk instead of generating on-the-fly.

These classes replace the IterableDataset implementations with regular Dataset classes
that load precomputed data from .npz files generated by generate_precomputed_data.py
"""

import torch
import numpy as np
from torch.utils.data import Dataset, DataLoader
import os
import glob
from typing import List, Tuple, Optional


def filter_valid_precomputed_files(file_list: List[str]) -> List[str]:
    """
    Filter a list of files to only include valid precomputed dataset files.
    
    Valid files must:
    - End with '.npz' 
    - Not contain '.tmp' in the filename (excludes temporary/incomplete files)
    
    Args:
        file_list: List of file paths to filter
        
    Returns:
        List of valid precomputed dataset file paths
    """
    valid_files = []
    for file_path in file_list:
        filename = os.path.basename(file_path)
        # Only include .npz files that don't contain '.tmp' anywhere in the filename
        if filename.endswith('.npz') and '.tmp' not in filename:
            valid_files.append(file_path)
    return valid_files


class PrecomputedDataset(Dataset):
    """
    Dataset class for loading precomputed theta parameters and corresponding event data.
    
    This replaces the on-the-fly data generation with disk-based loading for:
    - Better reproducibility
    - Faster training iterations  
    - Easier debugging
    """
    
    def __init__(self, data_dir: str, problem: str, shuffle: bool = True):
        """
        Initialize dataset from precomputed data files.
        
        Args:
            data_dir: Directory containing .npz files
            problem: Problem type ('gaussian', 'simplified_dis', 'realistic_dis', 'mceg')
            shuffle: Whether to shuffle the dataset indices
        """
        self.data_dir = data_dir
        self.problem = problem
        self.shuffle = shuffle
        
        # Find all matching data files
        pattern = os.path.join(data_dir, f"{problem}_*.npz")
        all_data_files = sorted(glob.glob(pattern))
        
        # Filter out temporary/incomplete files
        self.data_files = filter_valid_precomputed_files(all_data_files)
        
        if not self.data_files:
            if all_data_files:
                temp_files = [f for f in all_data_files if f not in self.data_files]
                print(f"Warning: Found {len(temp_files)} temporary/incomplete files for problem '{problem}' in {data_dir}: {temp_files}")
                print(f"Only complete .npz files (without .tmp) are considered valid precomputed data.")
            raise FileNotFoundError(f"No valid precomputed data files found for problem '{problem}' in {data_dir}")
        
        print(f"Found {len(self.data_files)} valid data files for {problem}")
        if len(all_data_files) > len(self.data_files):
            ignored_files = [f for f in all_data_files if f not in self.data_files]
            print(f"Ignored {len(ignored_files)} temporary/incomplete files: {ignored_files}")
        
        # Load all data into memory (assuming datasets are not too large)
        self._load_data()
        
        # Create indices for shuffling
        self.indices = np.arange(len(self.thetas))
        if self.shuffle:
            np.random.shuffle(self.indices)
    
    def _load_data(self):
        """Load all data files into memory."""
        all_thetas = []
        all_events = []
        
        for data_file in self.data_files:
            try:
                data = np.load(data_file)
                
                # Verify this is the correct problem type
                if 'problem' in data:
                    file_problem = str(data['problem'].item())
                    if file_problem != self.problem:
                        print(f"Warning: File {data_file} is for problem '{file_problem}', expected '{self.problem}'")
                        continue
                
                thetas = data['thetas']  # [num_samples, theta_dim]
                events = data['events']  # [num_samples, n_repeat, num_events, feature_dim]
                
                all_thetas.append(thetas)
                all_events.append(events)
                
                print(f"Loaded {data_file}: thetas {thetas.shape}, events {events.shape}")
                
            except Exception as e:
                print(f"Error loading {data_file}: {e}")
                continue
        
        if not all_thetas:
            raise ValueError(f"No valid data loaded for problem {self.problem}")
        
        # Concatenate all data
        self.thetas = np.concatenate(all_thetas, axis=0)
        self.events = np.concatenate(all_events, axis=0)
        
        print(f"Total loaded data: thetas {self.thetas.shape}, events {self.events.shape}")
        
        # Convert to tensors
        self.thetas = torch.from_numpy(self.thetas).float()
        self.events = torch.from_numpy(self.events).float()
    
    def __len__(self):
        return len(self.thetas)
    
    def __getitem__(self, idx):
        """
        Get a single sample.
        
        Returns:
            theta: Parameter vector [theta_dim]
            events: Event data [n_repeat, num_events, feature_dim]
        """
        actual_idx = self.indices[idx]
        theta = self.thetas[actual_idx]
        events = self.events[actual_idx]
        return theta, events
    
    def get_metadata(self):
        """Get metadata about the dataset."""
        return {
            'problem': self.problem,
            'num_samples': len(self.thetas),
            'theta_dim': self.thetas.shape[1],
            'n_repeat': self.events.shape[1],
            'num_events': self.events.shape[2],
            'feature_dim': self.events.shape[3],
            'data_files': self.data_files
        }


class DistributedPrecomputedDataset(PrecomputedDataset):
    """
    Distributed version of PrecomputedDataset that splits data across ranks.
    
    This maintains compatibility with the distributed training setup.
    """
    
    def __init__(self, data_dir: str, problem: str, rank: int, world_size: int, shuffle: bool = True):
        """
        Initialize distributed dataset.
        
        Args:
            data_dir: Directory containing .npz files
            problem: Problem type
            rank: Current process rank
            world_size: Total number of processes
            shuffle: Whether to shuffle before splitting
        """
        self.rank = rank
        self.world_size = world_size
        
        # Load full dataset first
        super().__init__(data_dir, problem, shuffle=shuffle)
        
        # Split data across ranks
        self._split_data_for_rank()
    
    def _split_data_for_rank(self):
        """Split the loaded data for this rank."""
        total_samples = len(self.thetas)
        samples_per_rank = total_samples // self.world_size
        remainder = total_samples % self.world_size
        
        # Calculate start and end indices for this rank
        start_idx = self.rank * samples_per_rank
        end_idx = start_idx + samples_per_rank
        
        # Distribute remainder among first few ranks
        if self.rank < remainder:
            start_idx += self.rank
            end_idx += self.rank + 1
        else:
            start_idx += remainder
            end_idx += remainder
        
        # Slice data for this rank
        self.thetas = self.thetas[start_idx:end_idx]
        self.events = self.events[start_idx:end_idx]
        
        # Update indices
        self.indices = np.arange(len(self.thetas))
        if self.shuffle:
            # Use rank-specific seed for reproducible shuffling
            np.random.seed(42 + self.rank)
            np.random.shuffle(self.indices)
        
        print(f"Rank {self.rank}: samples {start_idx}-{end_idx-1} ({len(self.thetas)} total)")


def create_precomputed_dataloader(
    data_dir: str, 
    problem: str, 
    batch_size: int = 32,
    shuffle: bool = True,
    num_workers: int = 0,
    rank: int = 0,
    world_size: int = 1,
    **kwargs
) -> DataLoader:
    """
    Create a DataLoader for precomputed data.
    
    Args:
        data_dir: Directory containing precomputed .npz files
        problem: Problem type ('gaussian', 'simplified_dis', etc.)
        batch_size: Batch size for training
        shuffle: Whether to shuffle data
        num_workers: Number of worker processes
        rank: Process rank for distributed training
        world_size: Total number of processes
        **kwargs: Additional arguments for DataLoader
        
    Returns:
        DataLoader instance
    """
    if world_size > 1:
        dataset = DistributedPrecomputedDataset(data_dir, problem, rank, world_size, shuffle)
    else:
        dataset = PrecomputedDataset(data_dir, problem, shuffle)
    
    # Don't shuffle in DataLoader if we're handling it in Dataset
    dataloader_shuffle = False
    
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=dataloader_shuffle,
        num_workers=num_workers,
        pin_memory=True,
        **kwargs
    )
    
    return dataloader


def test_precomputed_dataset():
    """Test function for the precomputed dataset classes."""
    import tempfile
    import sys
    import os
    
    # Add the main directory to path
    sys.path.append('/home/runner/work/PDFParameterInference/PDFParameterInference')
    
    print("Testing PrecomputedDataset...")
    
    # Test with the data we generated earlier
    test_dir = "/tmp/test_precomputed"
    
    if not os.path.exists(test_dir):
        print("No test data found. Run generate_precomputed_data.py first.")
        return
    
    # Test Gaussian dataset
    try:
        dataset = PrecomputedDataset(test_dir, "gaussian", shuffle=True)
        print(f"Gaussian dataset loaded: {len(dataset)} samples")
        print(f"Metadata: {dataset.get_metadata()}")
        
        # Test dataloader
        dataloader = DataLoader(dataset, batch_size=4, shuffle=False)
        for i, (theta, events) in enumerate(dataloader):
            print(f"Batch {i}: theta {theta.shape}, events {events.shape}")
            if i >= 1:  # Just test first couple batches
                break
        
        print("Gaussian dataset test passed!")
        
    except Exception as e:
        print(f"Gaussian dataset test failed: {e}")
    
    # Test SimplifiedDIS dataset
    try:
        dataset = PrecomputedDataset(test_dir, "simplified_dis", shuffle=True)
        print(f"SimplifiedDIS dataset loaded: {len(dataset)} samples")
        print(f"Metadata: {dataset.get_metadata()}")
        
        # Test distributed version
        dist_dataset = DistributedPrecomputedDataset(test_dir, "simplified_dis", rank=0, world_size=2)
        print(f"Distributed dataset (rank 0/2): {len(dist_dataset)} samples")
        
        print("SimplifiedDIS dataset test passed!")
        
    except Exception as e:
        print(f"SimplifiedDIS dataset test failed: {e}")
    
    print("All tests completed!")


if __name__ == "__main__":
    test_precomputed_dataset()