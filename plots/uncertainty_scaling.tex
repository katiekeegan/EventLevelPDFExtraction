
\section{Uncertainty Scaling with Number of Events}

This figure demonstrates how parameter estimation uncertainty decreases as the number of 
events per experiment increases, illustrating the fundamental statistical relationship 
between sample size and estimation precision.

\textbf{Theoretical Expectation:}

For most well-behaved estimators, the standard error should scale as:
$$\sigma_{\hat{\theta}} \propto \frac{1}{\sqrt{N}}$$

where $N$ is the number of events. This follows from the Central Limit Theorem and 
the fact that the variance of a sample mean decreases as $1/N$.

\textbf{Visualization Elements:}

\begin{itemize}
\item \textbf{Observed scaling} (blue circles): Empirical standard deviations computed from 
  multiple parameter estimates at each event count
\item \textbf{Theoretical scaling} (red dashed): Reference $1/\sqrt{N}$ scaling normalized 
  to match the observed data at a reference point
\item \textbf{Fitted scaling}: Power-law fit to the observed data showing the actual scaling exponent
\end{itemize}

\textbf{Experimental Procedure:}
\begin{enumerate}
\item For each event count $N \in \{100, 500, 1000, 5000, 10000\}$:
\item Generate 20 independent datasets with $N$ events each
\item Estimate parameters $\hat{\theta}$ for each dataset
\item Compute standard deviation across the 20 estimates
\end{enumerate}

\textbf{Interpretation:}
\begin{itemize}
\item \textbf{Adherence to theory}: Parameters following $N^{-0.5}$ scaling indicate well-behaved 
  estimation with no systematic issues
\item \textbf{Deviations from theory}: Faster or slower scaling may indicate systematic effects, 
  model misspecification, or numerical issues
\item \textbf{Practical implications}: The scaling relationship helps predict how much data is 
  needed to achieve desired precision levels
\end{itemize}

This analysis is crucial for experimental design, helping determine optimal data collection 
strategies and computational resource allocation.
